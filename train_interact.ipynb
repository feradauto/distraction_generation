{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c7c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "from configuration import Configuration\n",
    "from configuration import CONSTANTS as C\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1786ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "    \"\"\"    \n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# define a rich console logger\n",
    "\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "    table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "\n",
    "def create_model_dir(experiment_main_dir, experiment_id, model_summary):\n",
    "    \"\"\"\n",
    "    Create a new model directory.\n",
    "    :param experiment_main_dir: Where all experiments are stored.\n",
    "    :param experiment_id: The ID of this experiment.\n",
    "    :param model_summary: A summary string of the model.\n",
    "    :return: A directory where we can store model logs. Raises an exception if the model directory already exists.\n",
    "    \"\"\"\n",
    "    model_name = \"{}-{}\".format(experiment_id, model_summary)\n",
    "    model_dir = os.path.join(experiment_main_dir, model_name)\n",
    "    if os.path.exists(model_dir):\n",
    "        raise ValueError(\"Model directory already exists {}\".format(model_dir))\n",
    "    os.makedirs(model_dir)\n",
    "    return model_dir\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer,writer,global_step,records):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"loss\", loss, global_step)\n",
    "        \n",
    "        \n",
    "        ### measure bleu\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "          input_ids = ids,\n",
    "          attention_mask = mask, \n",
    "          max_length=150, \n",
    "          num_beams=2,\n",
    "          repetition_penalty=2.5, \n",
    "          length_penalty=1.0, \n",
    "          early_stopping=True\n",
    "          )\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        actuals.extend(target)\n",
    "\n",
    "        temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "\n",
    "        val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "        gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "        distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x)}).reset_index()\n",
    "\n",
    "        dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "        aa=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1)\n",
    "        print(aa)\n",
    "        dist_compare['bleu']=aa\n",
    "        #dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "        bleu_3=dist_compare.bleu.mean()\n",
    "        model.train()\n",
    "        writer.add_scalar(\"bleu3\", bleu_3, global_step)\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader,writer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            #writer.add_scalar(\"loss_validation\", float(loss.detach().numpy()), global_step)\n",
    "            #global_step += 1\n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bb8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    model_params={\n",
    "        \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "        \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
    "        \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
    "        \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "        \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "        \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "        \"MAX_SOURCE_TEXT_LENGTH\":1200,  # max length of source text\n",
    "        \"MAX_TARGET_TEXT_LENGTH\":200,   # max length of target text\n",
    "        \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    source_text='text'\n",
    "    target_text='distractor'\n",
    "    model_params=model_params\n",
    "    output_dir=\"outputs\"\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_updated.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records = json.loads(content)\n",
    "    records=pd.DataFrame(records)\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(C.DEVICE)\n",
    "\n",
    "\n",
    "    records=records.assign(question=records.question.str.join(' '))\n",
    "    records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "    records=records.assign(article=records.article.str.join(' '))\n",
    "    records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "\n",
    "    records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "\n",
    "    records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "\n",
    "    records=records.loc[:,['text','distractor']]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=records.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=records.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "\n",
    "    loader=training_loader\n",
    "    experiment_id = int(time.time())\n",
    "    experiment_name = \"name\"\n",
    "    model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "        # Create Tensorboard logger.\n",
    "    global_step = 0\n",
    "    writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step)\n",
    "\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(C.DATA_DIR, \"experiments/predictions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d61ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    evaluation_strategy='steps',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate schedulery\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc98372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=./results, overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.STEPS, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=500, logging_dir=runs/May10_09-43-31_lo-login-02, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./results, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=0, mp_parameters=)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d93e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":1200,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":200,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "source_text='text'\n",
    "target_text='distractor'\n",
    "model_params=model_params\n",
    "output_dir=\"outputs\"\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_updated.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records = json.loads(content)\n",
    "records=pd.DataFrame(records)\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "model = model.to(C.DEVICE)\n",
    "\n",
    "\n",
    "records=records.assign(question=records.question.str.join(' '))\n",
    "records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "records=records.assign(article=records.article.str.join(' '))\n",
    "records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "\n",
    "records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "\n",
    "records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "\n",
    "records=records.loc[:,['text','distractor']]\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "train_size = 0.8\n",
    "train_dataset=records.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "val_dataset=records.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "  'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "  'shuffle': True,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "val_params = {\n",
    "  'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "  'shuffle': False,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "\n",
    "loader=training_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abf6e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['It makes the cities more beautiful .'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.loc[records.distractor.str.contains('It makes the cities more beautiful')].distractor.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d745126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = int(time.time())\n",
    "experiment_name = \"name\"\n",
    "model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "    # Create Tensorboard logger.\n",
    "global_step = 0\n",
    "writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "#for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "#    global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57655c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=C.DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075977f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-924e3364c05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       )\n\u001b[1;32m     30\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, **model_kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m             )\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1799\u001b[0m             )\n\u001b[1;32m   1800\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                 \u001b[0;31m# need to set correct `past` for each of the four key / value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 reordered_layer_past_states = reordered_layer_past_states + (\n\u001b[0;32m-> 1631\u001b[0;31m                     \u001b[0mlayer_past_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m                 )\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for _,data in enumerate(loader, 0):\n",
    "    y = data['target_ids'].to(device, dtype = torch.long)\n",
    "    y_ids = y[:, :-1].contiguous()\n",
    "    lm_labels = y[:, 1:].clone().detach()\n",
    "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "    outputs = model.forward(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "    loss = outputs[0]\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(\"loss\", float(loss.detach().numpy()), global_step)\n",
    "    \n",
    "    ### measure bleu\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "      input_ids = ids,\n",
    "      attention_mask = mask, \n",
    "      max_length=150, \n",
    "      num_beams=2,\n",
    "      repetition_penalty=2.5, \n",
    "      length_penalty=1.0, \n",
    "      early_stopping=True\n",
    "      )\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "    predictions.extend(preds)\n",
    "    actuals.extend(target)\n",
    "\n",
    "    temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "\n",
    "    val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "    gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "    distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x)}).reset_index()\n",
    "\n",
    "    dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "\n",
    "    dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "    bleu_3=dist_compare.bleu.mean()\n",
    "    model.train()\n",
    "    writer.add_scalar(\"bleu3\", bleu_3, global_step)\n",
    "    \n",
    "    \n",
    "    global_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f308594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model after training\n",
    "path = os.path.join(output_dir, \"model_files\")\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "# evaluating test dataset\n",
    "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d624331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.read_csv(os.path.join(C.DATA_DIR, \"experiments/predictions.csv\"))\n",
    "\n",
    "val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "gen_dist=val.merge(final_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x)}).reset_index()\n",
    "\n",
    "dist_compare=distractors.merge(gen_dist,on=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0235666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f581a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2904623040609807"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.bleu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0588c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2904623040609807"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.bleu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c7f7e91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>distraction passage: On Easter Day , 1722 , Du...</td>\n",
       "      <td>[the Islanders built the moai to show off thei...</td>\n",
       "      <td>the first people to arrive on Easter Island ca...</td>\n",
       "      <td>0.47749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>distraction passage: On Easter Day , 1722 , Du...</td>\n",
       "      <td>[the Islanders built the moai to show off thei...</td>\n",
       "      <td>the first people to arrive on Easter Island ca...</td>\n",
       "      <td>0.47749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>distraction passage: On Easter Day , 1722 , Du...</td>\n",
       "      <td>[the Islanders built the moai to show off thei...</td>\n",
       "      <td>the first people to arrive on Easter Island ca...</td>\n",
       "      <td>0.47749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "626  distraction passage: On Easter Day , 1722 , Du...   \n",
       "627  distraction passage: On Easter Day , 1722 , Du...   \n",
       "628  distraction passage: On Easter Day , 1722 , Du...   \n",
       "\n",
       "                                           Actual Text  \\\n",
       "626  [the Islanders built the moai to show off thei...   \n",
       "627  [the Islanders built the moai to show off thei...   \n",
       "628  [the Islanders built the moai to show off thei...   \n",
       "\n",
       "                                        Generated Text     bleu  \n",
       "626  the first people to arrive on Easter Island ca...  0.47749  \n",
       "627  the first people to arrive on Easter Island ca...  0.47749  \n",
       "628  the first people to arrive on Easter Island ca...  0.47749  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare[dist_compare['text']==\"distraction passage: On Easter Day , 1722 , Dutch explorers    landed on Easter Island   . It was the first time that Easter Islanders had met people from the outside world . The strangers were about to discover something very strange themselves --that they were on an island with hundreds of huge stone statues   . The Dutch explorers wondered where the Islanders had come from and why and how they had built the statues . Now science is putting together the story .      The first people to arrive on the island came there around A.D. 700 . The society that developed there was based on fishing and farming to feed the population , which grew to 12,000 . Its success showed itself in a way that has become the island 's trademark   : hundreds of huge stone figures --the moai .      None of the moai was standing when scientists first arrived . People put them back up later ; but how had a Stone Age society ever made , moved and set them up there in the first place ? And why ?      There are nearly 900 moai on Easter Island , and while the questions about them remain unanswered , no one doubts the years of effort that must have gone into making them .      The real killer of the Easter Islanders came from across the ocean . After 1722 , it became popular for explorers to visit Easter Island , bringing diseases . The final blow    came in 1862 , when slave traders came from Peru and took away 1,500 people , one - third of the population . question: The passage implies that answer: in 1862 , before slave traders came , about 4,500 people were living on Easter Island\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d959ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the Islanders built the moai to show off their success',\n",
       " 'the Dutch explorers discovered how the moai had been built',\n",
       " 'the natives of Easter Island have been there for about 1,200 years']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.loc[626,'Actual Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b712510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95775337",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(Configuration.parse_cmd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df0efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
