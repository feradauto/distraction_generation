{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d83841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "from configuration import Configuration\n",
    "from configuration import CONSTANTS as C\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92df0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "    \"\"\"    \n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_model_dir(experiment_main_dir, experiment_id, model_summary):\n",
    "    \"\"\"\n",
    "    Create a new model directory.\n",
    "    :param experiment_main_dir: Where all experiments are stored.\n",
    "    :param experiment_id: The ID of this experiment.\n",
    "    :param model_summary: A summary string of the model.\n",
    "    :return: A directory where we can store model logs. Raises an exception if the model directory already exists.\n",
    "    \"\"\"\n",
    "    model_name = \"{}-{}\".format(experiment_id, model_summary)\n",
    "    model_dir = os.path.join(experiment_main_dir, model_name)\n",
    "    if os.path.exists(model_dir):\n",
    "        raise ValueError(\"Model directory already exists {}\".format(model_dir))\n",
    "    os.makedirs(model_dir)\n",
    "    return model_dir\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer,writer,global_step,records):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"loss\", loss, global_step)\n",
    "        \n",
    "        \n",
    "        ### measure bleu\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "          input_ids = ids,\n",
    "          attention_mask = mask, \n",
    "          max_length=150, \n",
    "          num_beams=2,\n",
    "          repetition_penalty=2.5, \n",
    "          length_penalty=1.0, \n",
    "          early_stopping=True\n",
    "          )\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        actuals.extend(target)\n",
    "\n",
    "        temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "\n",
    "        val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "        gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "        distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x)}).reset_index()\n",
    "\n",
    "        dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "        aa=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1)\n",
    "        dist_compare=dist_compare.assign(bleu=aa)\n",
    "        #dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "        bleu_3=dist_compare.bleu.mean()\n",
    "        model.train()\n",
    "        writer.add_scalar(\"bleu3\", bleu_3, global_step)\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader,writer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            #writer.add_scalar(\"loss_validation\", float(loss.detach().numpy()), global_step)\n",
    "            #global_step += 1\n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3833330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    model_params={\n",
    "        \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "        \"TRAIN_BATCH_SIZE\":4,          # training batch size\n",
    "        \"VALID_BATCH_SIZE\":4,          # validation batch size\n",
    "        \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "        \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "        \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "        \"MAX_SOURCE_TEXT_LENGTH\":1200,  # max length of source text\n",
    "        \"MAX_TARGET_TEXT_LENGTH\":200,   # max length of target text\n",
    "        \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    source_text='text'\n",
    "    target_text='distractor'\n",
    "    model_params=model_params\n",
    "\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_updated.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records = json.loads(content)\n",
    "    records=pd.DataFrame(records)\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(C.DEVICE)\n",
    "\n",
    "    ## format the input\n",
    "    records=records.assign(question=records.question.str.join(' '))\n",
    "    records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "    records=records.assign(article=records.article.str.join(' '))\n",
    "    records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "    records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "    records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "    records=records.loc[:,['text','distractor']]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=records.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=records.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "    \n",
    "    # Create Tensorboard logger.\n",
    "    experiment_id = int(time.time())\n",
    "    experiment_name = \"name\"\n",
    "    model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "        \n",
    "    global_step = 0\n",
    "    writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step,records)\n",
    "\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(model_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "taken-sheet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is Kevin\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, this is Kevin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":4,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":4,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":1200,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":200,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "source_text='text'\n",
    "target_text='distractor'\n",
    "model_params=model_params\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_updated.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records = json.loads(content)\n",
    "records=pd.DataFrame(records)\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "model = model.to(C.DEVICE)\n",
    "\n",
    "## format the input\n",
    "records=records.assign(question=records.question.str.join(' '))\n",
    "records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "records=records.assign(article=records.article.str.join(' '))\n",
    "records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "records=records.loc[:,['text','distractor']]\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "train_size = 0.8\n",
    "train_dataset=records.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "val_dataset=records.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "  'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "  'shuffle': True,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "val_params = {\n",
    "  'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "  'shuffle': False,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "# Create Tensorboard logger.\n",
    "experiment_id = int(time.time())\n",
    "experiment_name = \"name\"\n",
    "model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "\n",
    "global_step = 0\n",
    "writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "    global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step,records)\n",
    "\n",
    "#Saving the model after training\n",
    "path = os.path.join(model_dir, \"model_files\")\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "# evaluating test dataset\n",
    "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(Configuration.parse_cmd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
