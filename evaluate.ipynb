{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d83841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "from configuration import Configuration\n",
    "from configuration import CONSTANTS as C\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch import cuda\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11a45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "    \"\"\"    \n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len,answer_len, source_text, target_text,answer_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.ans_len = answer_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "        self.answer_text = self.data[answer_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "        answer_text = str(self.answer_text[index])\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "        answer_text = ' '.join(answer_text.split())\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        answer = self.tokenizer.batch_encode_plus([answer_text], max_length= self.ans_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "        answer_ids = answer['input_ids'].squeeze()\n",
    "        answer_mask = answer['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long),\n",
    "            'answer_ids': answer_ids.to(dtype=torch.long),\n",
    "            'answer_mask': answer_mask.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_model_dir(experiment_main_dir, experiment_id, model_summary):\n",
    "    \"\"\"\n",
    "    Create a new model directory.\n",
    "    :param experiment_main_dir: Where all experiments are stored.\n",
    "    :param experiment_id: The ID of this experiment.\n",
    "    :param model_summary: A summary string of the model.\n",
    "    :return: A directory where we can store model logs. Raises an exception if the model directory already exists.\n",
    "    \"\"\"\n",
    "    model_name = \"{}-{}\".format(experiment_id, model_summary)\n",
    "    model_dir = os.path.join(experiment_main_dir, model_name)\n",
    "    if os.path.exists(model_dir):\n",
    "        raise ValueError(\"Model directory already exists {}\".format(model_dir))\n",
    "    os.makedirs(model_dir)\n",
    "    return model_dir\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer,writer,global_step,records,model_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    c=0\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        print(\"mem\",torch.cuda.memory_allocated(device=C.DEVICE))\n",
    "        c=c+1\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        ans_str = data['answer_ids'].to(device, dtype = torch.long)\n",
    "        ans_mask = data['answer_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids,\n",
    "                        labels=lm_labels,answer_str=ans_str,answer_mask=ans_mask,tokenizer=tokenizer,c=c)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        #print(\"preds\",outputs[\"pred_ids\"])\n",
    "        #preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs[\"pred_ids\"]]\n",
    "        #print(preds)\n",
    "        #print(\"ans\",outputs[\"ans_ids\"])\n",
    "        #an = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs[\"ans_ids\"]]\n",
    "        #print(an)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"loss\", loss, global_step)\n",
    "        \n",
    "        \n",
    "        ### measure bleu\n",
    "        if c%10==0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            actuals = []\n",
    "            num_dist=[]\n",
    "            ##outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=3,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True,\n",
    "            num_return_sequences=3,\n",
    "              )\n",
    "            print(generated_ids.shape)\n",
    "            print(generated_ids)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
    "            print(preds)\n",
    "            print(target)\n",
    "            predictions.extend(preds)\n",
    "            for tt in target:\n",
    "                print(tt)\n",
    "                actuals.extend([tt,tt,tt])\n",
    "                num_dist.extend([1,2,3])\n",
    "                print(\"actualslen\",len(actuals))\n",
    "                print(actuals)\n",
    "            print(len(actuals))\n",
    "            print(len(predictions))\n",
    "            print(num_dist)\n",
    "            temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals,'Num distractor':num_dist})\n",
    "            print(temp_df.head())\n",
    "            val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "            gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text','Num distractor']]\n",
    "\n",
    "            distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "            dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "            dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()\n",
    "            dist_compare=dist_compare.assign(bleu1=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(1, 0, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "            dist_compare=dist_compare.assign(bleu2=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "            dist_compare=dist_compare.assign(bleu3=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "            dist_compare=dist_compare.assign(bleu4=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 0, 1),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "            \n",
    "            for i in range(1,4):\n",
    "                bleu_1=dist_compare.loc[dist_compare['Num distractor']==i].bleu1.mean()\n",
    "                bleu_2=dist_compare.loc[dist_compare['Num distractor']==i].bleu2.mean()\n",
    "                bleu_3=dist_compare.loc[dist_compare['Num distractor']==i].bleu3.mean()\n",
    "                bleu_4=dist_compare.loc[dist_compare['Num distractor']==i].bleu4.mean()\n",
    "                writer.add_scalar('bleu/distractor_{}/bleu_1'.format(i), bleu_1, global_step)\n",
    "                writer.add_scalar('bleu/distractor_{}/bleu_2'.format(i), bleu_2, global_step)\n",
    "                writer.add_scalar('bleu/distractor_{}/bleu_3'.format(i), bleu_3, global_step)\n",
    "                writer.add_scalar('bleu/distractor_{}/bleu_4'.format(i), bleu_4, global_step)\n",
    "            \n",
    "            \n",
    "            bleu_1=dist_compare.bleu1.mean()\n",
    "            bleu_2=dist_compare.bleu2.mean()\n",
    "            bleu_3=dist_compare.bleu3.mean()\n",
    "            bleu_4=dist_compare.bleu4.mean()\n",
    "            writer.add_scalar(\"bleu/distractor_gen/bleu_1\", bleu_1, global_step)\n",
    "            writer.add_scalar(\"bleu/distractor_gen/bleu_2\", bleu_2, global_step)\n",
    "            writer.add_scalar(\"bleu/distractor_gen/bleu_3\", bleu_3, global_step)\n",
    "            writer.add_scalar(\"bleu/distractor_gen/bleu_4\", bleu_4, global_step)\n",
    "            \n",
    "            if c%1000==0:\n",
    "                path = os.path.join(model_dir, \"model_files\")\n",
    "                model.save_pretrained(path)\n",
    "                tokenizer.save_pretrained(path)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader,writer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    num_dist=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=3,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True,\n",
    "                num_return_sequences=3,\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
    "            predictions.extend(preds)\n",
    "            for tt in target:\n",
    "                actuals.extend([tt,tt,tt])\n",
    "                num_dist.extend([1,2,3])\n",
    "\n",
    "        temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals,'Num distractor':num_dist})\n",
    "        val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "        gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text','Num distractor']]\n",
    "\n",
    "        distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "        dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "        dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()\n",
    "        dist_compare=dist_compare.assign(bleu1=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(1, 0, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu2=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu3=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu4=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 0, 1),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "\n",
    "        for i in range(1,4):\n",
    "            bleu_1=dist_compare.loc[dist_compare['Num distractor']==i].bleu1.mean()\n",
    "            bleu_2=dist_compare.loc[dist_compare['Num distractor']==i].bleu2.mean()\n",
    "            bleu_3=dist_compare.loc[dist_compare['Num distractor']==i].bleu3.mean()\n",
    "            bleu_4=dist_compare.loc[dist_compare['Num distractor']==i].bleu4.mean()\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_1'.format(i), bleu_1, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_2'.format(i), bleu_2, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_3'.format(i), bleu_3, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_4'.format(i), bleu_4, global_step)\n",
    "\n",
    "\n",
    "        bleu_1=dist_compare.bleu1.mean()\n",
    "        bleu_2=dist_compare.bleu2.mean()\n",
    "        bleu_3=dist_compare.bleu3.mean()\n",
    "        bleu_4=dist_compare.bleu4.mean()\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_1\", bleu_1, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_2\", bleu_2, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_3\", bleu_3, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_4\", bleu_4, global_step)\n",
    "    \n",
    "            \n",
    "    return dist_compare\n",
    "\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    model_params={\n",
    "        \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "        \"TRAIN_BATCH_SIZE\":2,          # training batch size\n",
    "        \"VALID_BATCH_SIZE\":2,          # validation batch size\n",
    "        \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "        \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "        \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "        \"MAX_SOURCE_TEXT_LENGTH\":300,  # max length of source text\n",
    "        \"MAX_TARGET_TEXT_LENGTH\":301,   # max length of target text\n",
    "        \"MAX_ANSWER_LENGTH\":300,   # max length of answer text\n",
    "        \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    source_text='text'\n",
    "    target_text='distractor'\n",
    "    answer_text='answer_text'\n",
    "    model_params=model_params\n",
    "\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_train_original.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records = json.loads(content)\n",
    "    records=pd.DataFrame(records)\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(C.DEVICE)\n",
    "    \n",
    "    ## format the input\n",
    "    records=records.assign(question=records.question.str.join(' '))\n",
    "    records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "    records=records.assign(article=records.article.str.join(' '))\n",
    "    records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "    records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "    records=records.assign(text=\"dist q: \"+records.question+\" a: \"+records.answer_text+\" p: \"+records.article)\n",
    "    records=records.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_original.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records_test = json.loads(content)\n",
    "    records_test=pd.DataFrame(records_test)\n",
    "\n",
    "    ## format the input\n",
    "    records_test=records_test.assign(question=records_test.question.str.join(' '))\n",
    "    records_test=records_test.assign(distractor=records_test.distractor.str.join(' '))\n",
    "    records_test=records_test.assign(article=records_test.article.str.join(' '))\n",
    "    records_test=records_test.assign(answer_text=records_test.answer_text.str.join(' '))\n",
    "    records_test=records_test.loc[:,['article','question','answer_text','distractor']]\n",
    "    records_test=records_test.assign(text=\"dist q: \"+records_test.question+\" a: \"+records_test.answer_text+\" p: \"+records_test.article)\n",
    "    records_test=records_test.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    val_dataset=records_test\n",
    "    train_dataset = records\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "    \n",
    "    # Create Tensorboard logger.\n",
    "    experiment_id = int(time.time())\n",
    "    experiment_name = \"name\"\n",
    "    model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "        \n",
    "    global_step = 0\n",
    "    writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "    \n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step,records,model_dir)\n",
    "\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(model_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cf4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader,writer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    num_dist=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=3,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True,\n",
    "                num_return_sequences=3,\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
    "            predictions.extend(preds)\n",
    "            for tt in target:\n",
    "                actuals.extend([tt,tt,tt])\n",
    "                num_dist.extend([1,2,3])\n",
    "\n",
    "        temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals,'Num distractor':num_dist})\n",
    "        val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "        gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text','Num distractor']]\n",
    "\n",
    "        distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "        dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "        dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()\n",
    "        dist_compare=dist_compare.assign(bleu1=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(1, 0, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu2=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu3=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "        dist_compare=dist_compare.assign(bleu4=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 0, 1),smoothing_function=SmoothingFunction().method1),axis=1))\n",
    "\n",
    "        for i in range(1,4):\n",
    "            bleu_1=dist_compare.loc[dist_compare['Num distractor']==i].bleu1.mean()\n",
    "            bleu_2=dist_compare.loc[dist_compare['Num distractor']==i].bleu2.mean()\n",
    "            bleu_3=dist_compare.loc[dist_compare['Num distractor']==i].bleu3.mean()\n",
    "            bleu_4=dist_compare.loc[dist_compare['Num distractor']==i].bleu4.mean()\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_1'.format(i), bleu_1, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_2'.format(i), bleu_2, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_3'.format(i), bleu_3, global_step)\n",
    "            writer.add_scalar('val/bleu/distractor_{}/bleu_4'.format(i), bleu_4, global_step)\n",
    "\n",
    "\n",
    "        bleu_1=dist_compare.bleu1.mean()\n",
    "        bleu_2=dist_compare.bleu2.mean()\n",
    "        bleu_3=dist_compare.bleu3.mean()\n",
    "        bleu_4=dist_compare.bleu4.mean()\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_1\", bleu_1, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_2\", bleu_2, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_3\", bleu_3, global_step)\n",
    "        writer.add_scalar(\"val/bleu/distractor_gen/bleu_4\", bleu_4, global_step)\n",
    "            \n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8ac124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_params={\n",
    "    \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":2,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":2,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":900,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":901,   # max length of target text\n",
    "    \"MAX_ANSWER_LENGTH\":900,   # max length of answer text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "source_text='text'\n",
    "target_text='distractor'\n",
    "answer_text='answer_text'\n",
    "model_params=model_params\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_train_original.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records = json.loads(content)\n",
    "records=pd.DataFrame(records)\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "model = model.to(C.DEVICE)\n",
    "\n",
    "## format the input\n",
    "records=records.assign(question=records.question.str.join(' '))\n",
    "records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "records=records.assign(article=records.article.str.join(' '))\n",
    "records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "records=records.assign(text=\"dist q: \"+records.question+\" a: \"+records.answer_text+\" p: \"+records.article)\n",
    "records=records.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_original.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records_test = json.loads(content)\n",
    "records_test=pd.DataFrame(records_test)\n",
    "\n",
    "## format the input\n",
    "records_test=records_test.assign(question=records_test.question.str.join(' '))\n",
    "records_test=records_test.assign(distractor=records_test.distractor.str.join(' '))\n",
    "records_test=records_test.assign(article=records_test.article.str.join(' '))\n",
    "records_test=records_test.assign(answer_text=records_test.answer_text.str.join(' '))\n",
    "records_test=records_test.loc[:,['article','question','answer_text','distractor']]\n",
    "records_test=records_test.assign(text=\"dist q: \"+records_test.question+\" a: \"+records_test.answer_text+\" p: \"+records_test.article)\n",
    "records_test=records_test.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "val_dataset=records_test\n",
    "train_dataset = records\n",
    "\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "\n",
    "\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "  'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "  'shuffle': True,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "val_params = {\n",
    "  'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "  'shuffle': False,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eafb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dir(experiment_dir, model_id):\n",
    "    \"\"\"Return the directory in `experiment_dir` that contains the given `model_id` string.\"\"\"\n",
    "    model_dir = glob.glob(os.path.join(experiment_dir, str(model_id) + \"-*\"), recursive=False)\n",
    "    return None if len(model_dir) == 0 else model_dir[0]\n",
    "\n",
    "def get_model_config(model_id):\n",
    "    model_id = model_id\n",
    "    model_dir = get_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), model_id)\n",
    "    model_config = 0#Configuration.from_json(os.path.join(model_dir, 'config.json'))\n",
    "    return model_config, model_dir\n",
    "\n",
    "def load_model(model_id):\n",
    "    model_config, model_dir = get_model_config(model_id)\n",
    "    path = os.path.join(model_dir, \"model_files\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "\n",
    "    model.to(C.DEVICE)\n",
    "\n",
    "    return model,tokenizer, model_config, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68b0e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer, model_config, model_dir = load_model(1625154455)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df55310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join(model_dir, 'logs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfba18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    final_df = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "    final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6968c5b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/home/fgonzalez/nlp/data/experiments/1625154455-name/predictions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-651779b72548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib64/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/home/fgonzalez/nlp/data/experiments/1625154455-name/predictions.csv'"
     ]
    }
   ],
   "source": [
    "final_df=pd.read_csv(os.path.join(model_dir, 'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e56727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Num distractor    1.999952\n",
       "bleu1             0.229085\n",
       "bleu2             0.076467\n",
       "bleu3             0.041808\n",
       "bleu4             0.030668\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "1625497725    lambda 0.3 normal cosine\n",
    "1625497726    lambda 0.4 normal cosine\n",
    "1625497756    lambda 0.2 normal cosine\n",
    "1625665307    lambda 0.1 normal cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1395afcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num distractor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.253613</td>\n",
       "      <td>0.090932</td>\n",
       "      <td>0.051882</td>\n",
       "      <td>0.038624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246943</td>\n",
       "      <td>0.082840</td>\n",
       "      <td>0.046607</td>\n",
       "      <td>0.034424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.195244</td>\n",
       "      <td>0.063403</td>\n",
       "      <td>0.033490</td>\n",
       "      <td>0.025785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bleu1     bleu2     bleu3     bleu4\n",
       "Num distractor                                        \n",
       "1               0.253613  0.090932  0.051882  0.038624\n",
       "2               0.246943  0.082840  0.046607  0.034424\n",
       "3               0.195244  0.063403  0.033490  0.025785"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1625497726 0.4\n",
    "final_df.groupby(['Num distractor']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7993960a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num distractor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.254536</td>\n",
       "      <td>0.090609</td>\n",
       "      <td>0.051279</td>\n",
       "      <td>0.037286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248582</td>\n",
       "      <td>0.081285</td>\n",
       "      <td>0.043693</td>\n",
       "      <td>0.032288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184080</td>\n",
       "      <td>0.057492</td>\n",
       "      <td>0.030443</td>\n",
       "      <td>0.022425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bleu1     bleu2     bleu3     bleu4\n",
       "Num distractor                                        \n",
       "1               0.254536  0.090609  0.051279  0.037286\n",
       "2               0.248582  0.081285  0.043693  0.032288\n",
       "3               0.184080  0.057492  0.030443  0.022425"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1625497725 0.3\n",
    "final_df.groupby(['Num distractor']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1f26a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num distractor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237260</td>\n",
       "      <td>0.083794</td>\n",
       "      <td>0.047737</td>\n",
       "      <td>0.034073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.242868</td>\n",
       "      <td>0.080669</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>0.033270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200969</td>\n",
       "      <td>0.065399</td>\n",
       "      <td>0.035270</td>\n",
       "      <td>0.026071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bleu1     bleu2     bleu3     bleu4\n",
       "Num distractor                                        \n",
       "1               0.237260  0.083794  0.047737  0.034073\n",
       "2               0.242868  0.080669  0.044823  0.033270\n",
       "3               0.200969  0.065399  0.035270  0.026071"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1625497756 0.2\n",
    "final_df.groupby(['Num distractor']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab8ff735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num distractor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.230350</td>\n",
       "      <td>0.082875</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.251306</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>0.035110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212373</td>\n",
       "      <td>0.070870</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.028427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bleu1     bleu2     bleu3     bleu4\n",
       "Num distractor                                        \n",
       "1               0.230350  0.082875  0.048152  0.034500\n",
       "2               0.251306  0.086232  0.047511  0.035110\n",
       "3               0.212373  0.070870  0.038128  0.028427"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1625665307\n",
    "final_df.groupby(['Num distractor']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1625154455\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32ca69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a6208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903845fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, distractor]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[records.text=='dist q: McCulloch and his group used_in their research . a: 5 dogs and 169 people p: Dogs have long been used to find explosives and drugs . Now , a new study shows that man \\'s best friend can also help to find lung and breast cancer , researchers report in integrative Cancer Therapies . The findings show that trained ordinary household dogs can detect early -- stage lung and breast cancers by smelling the breath samples of patients . Researchers have found that cancer cells send out molecules different from those of healthy ones , and that might be sensed by smell by the highly sensitive dog \\'s nose . For the study , five dogs were trained by a professional instructor to respond differently to breath samples of healthy and cancer patients.\"The dogs learned to sit or lie down in front of cancer patient samples and to ignore control samples through the method of food reward , \" McCulloch explained . After a period of training , researchers tested the animals\\'ability to distinguish cancer patients from controls . The animals were given breath samples from 55 patients with lung cancer,3 1 with breast cancer and 83 healthy controls who were not included in the original training period . McCulloch \\'s group found that the dogs were able to correctly distinguish the breath samples of cancer patients from those of the control subjects in about 90 percent of the cases . The dogs were also able to detecting early - stage lung and breast cancers . \" These results show that there is hope for early detection,\"McCulloch said . The re - searches are planning to conduct further studies on the breath composition of cancer patients to possibly design an electronic device that can do the dogs\\'job.\"A dog \\'s nose is so powerful it can detect odors 10 000 to 100 000 times better than a human nose can . I hope people will be interested in doing this research,\"McCulloch added,\"It shows that there is definitely something out there . \"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc772f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = final_df\n",
    "\n",
    "val=records_test_fil.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']].drop_duplicates()\n",
    "\n",
    "distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "dist_compare=distractors.merge(gen_dist,on=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7876f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca08a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(1, 0, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b2=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b3=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b4=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 0, 1),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "dist_compare=dist_compare.assign(bleu1=b1)\n",
    "dist_compare=dist_compare.assign(bleu2=b2)\n",
    "dist_compare=dist_compare.assign(bleu3=b3)\n",
    "dist_compare=dist_compare.assign(bleu4=b4)\n",
    "#dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "bleu_1=dist_compare.bleu1.mean()\n",
    "bleu_2=dist_compare.bleu2.mean()\n",
    "bleu_3=dist_compare.bleu3.mean()\n",
    "bleu_4=dist_compare.bleu4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91598d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07887480099137983"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c749acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dist q: \" ... Old is suddenly in \" ( Line 1 , ...</td>\n",
       "      <td>[[America, has, suddenly, become, a, nation, o...</td>\n",
       "      <td>[\", _, \"]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>dist q: The author takes Albert Einstein as an...</td>\n",
       "      <td>[[tell, the, fact, that, Einstein, was, well, ...</td>\n",
       "      <td>[a, messy, shop, front]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>dist q: The author takes Albert Einstein as an...</td>\n",
       "      <td>[[tell, the, fact, that, Einstein, was, well, ...</td>\n",
       "      <td>[a, messy, shop, front]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>dist q: The author used wolves as an example t...</td>\n",
       "      <td>[[explain, the, cruel, side, of, group, -, liv...</td>\n",
       "      <td>[wolves, as, a, model, for, developing, and, m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>dist q: The author used wolves as an example t...</td>\n",
       "      <td>[[explain, the, cruel, side, of, group, -, liv...</td>\n",
       "      <td>[wolves, as, a, model, for, developing, and, m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>dist q: If you are interested in history you w...</td>\n",
       "      <td>[[Rocky, Mountain, National, Park], [Black, Ca...</td>\n",
       "      <td>[Rocky, Mountain, National, Park]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>dist q: If you are interested in history you w...</td>\n",
       "      <td>[[Rocky, Mountain, National, Park], [Black, Ca...</td>\n",
       "      <td>[Rocky, Mountain, National, Park]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>dist q: A girl who likes yoga will go to a: Pe...</td>\n",
       "      <td>[[Camp, Jano, India], [Bay, Language, Academy]]</td>\n",
       "      <td>[Camp, Jano, India]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>dist q: The time of the recognition span can b...</td>\n",
       "      <td>[[one, 's, purpose, in, reading], [lighting, a...</td>\n",
       "      <td>[lighting, and, tiredness]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>dist q: According tothe study by the World Ban...</td>\n",
       "      <td>[[benefit, to, a, third, of, their, population...</td>\n",
       "      <td>[a, third, of, their, population]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     dist q: \" ... Old is suddenly in \" ( Line 1 , ...   \n",
       "3715  dist q: The author takes Albert Einstein as an...   \n",
       "3716  dist q: The author takes Albert Einstein as an...   \n",
       "3738  dist q: The author used wolves as an example t...   \n",
       "3739  dist q: The author used wolves as an example t...   \n",
       "...                                                 ...   \n",
       "1981  dist q: If you are interested in history you w...   \n",
       "1982  dist q: If you are interested in history you w...   \n",
       "93    dist q: A girl who likes yoga will go to a: Pe...   \n",
       "5075  dist q: The time of the recognition span can b...   \n",
       "756   dist q: According tothe study by the World Ban...   \n",
       "\n",
       "                                            Actual Text  \\\n",
       "0     [[America, has, suddenly, become, a, nation, o...   \n",
       "3715  [[tell, the, fact, that, Einstein, was, well, ...   \n",
       "3716  [[tell, the, fact, that, Einstein, was, well, ...   \n",
       "3738  [[explain, the, cruel, side, of, group, -, liv...   \n",
       "3739  [[explain, the, cruel, side, of, group, -, liv...   \n",
       "...                                                 ...   \n",
       "1981  [[Rocky, Mountain, National, Park], [Black, Ca...   \n",
       "1982  [[Rocky, Mountain, National, Park], [Black, Ca...   \n",
       "93      [[Camp, Jano, India], [Bay, Language, Academy]]   \n",
       "5075  [[one, 's, purpose, in, reading], [lighting, a...   \n",
       "756   [[benefit, to, a, third, of, their, population...   \n",
       "\n",
       "                                         Generated Text  bleu  \n",
       "0                                             [\", _, \"]   0.0  \n",
       "3715                            [a, messy, shop, front]   0.0  \n",
       "3716                            [a, messy, shop, front]   0.0  \n",
       "3738  [wolves, as, a, model, for, developing, and, m...   0.0  \n",
       "3739  [wolves, as, a, model, for, developing, and, m...   0.0  \n",
       "...                                                 ...   ...  \n",
       "1981                  [Rocky, Mountain, National, Park]   1.0  \n",
       "1982                  [Rocky, Mountain, National, Park]   1.0  \n",
       "93                                  [Camp, Jano, India]   1.0  \n",
       "5075                         [lighting, and, tiredness]   1.0  \n",
       "756                   [a, third, of, their, population]   1.0  \n",
       "\n",
       "[7500 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.sort_values('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5eef5019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>dist q: What would be the best title for the p...</td>\n",
       "      <td>[[Music, of, the, Mission, District], [The, Sp...</td>\n",
       "      <td>[The, Mission, District]</td>\n",
       "      <td>0.513417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>dist q: The research program is chiefly design...</td>\n",
       "      <td>[[high, school, advisers, from, Houston], [col...</td>\n",
       "      <td>[high, school, students]</td>\n",
       "      <td>0.513417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>dist q: We can judge from the Deloitte study t...</td>\n",
       "      <td>[[the, French, are, less, willing, to, buy, ec...</td>\n",
       "      <td>[French, holiday, shoppers, are, choosing, mor...</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>dist q: From the passage we can see that a: 99...</td>\n",
       "      <td>[[in, Europe, 94, billion, cubic, meters, of, ...</td>\n",
       "      <td>[94, billion, cubic, meters, of, methane, per,...</td>\n",
       "      <td>0.530714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>dist q: As a result of the ticketing mistake ,...</td>\n",
       "      <td>[[will, not, enjoy, the, synchronized, swimmin...</td>\n",
       "      <td>[the, men's, 100, m, final]</td>\n",
       "      <td>0.536256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>dist q: We can find the introduction to a prod...</td>\n",
       "      <td>[[Part, 1], [Part, 2], [Part, 3]]</td>\n",
       "      <td>[Part, 3]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>dist q: What did Paul have for dinner ? a: The...</td>\n",
       "      <td>[[The, soup, and, the, duck], [The, duck, ,, t...</td>\n",
       "      <td>[The, duck, and, the, soup]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>dist q: What does Sue like ? a: Playing games ...</td>\n",
       "      <td>[[Swimming, and, reading]]</td>\n",
       "      <td>[Swimming, and, reading]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>dist q: Lisa in this passage is the name of a:...</td>\n",
       "      <td>[[a, male, lion], [a, pride]]</td>\n",
       "      <td>[a, male, lion]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>dist q: You should especially protect yourself...</td>\n",
       "      <td>[[Jet, Ski, Tours, of, Miami], [15, thStreet, ...</td>\n",
       "      <td>[Jet, Ski, Tours, of, Miami]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "3236  dist q: What would be the best title for the p...   \n",
       "2416  dist q: The research program is chiefly design...   \n",
       "2869  dist q: We can judge from the Deloitte study t...   \n",
       "778   dist q: From the passage we can see that a: 99...   \n",
       "445   dist q: As a result of the ticketing mistake ,...   \n",
       "...                                                 ...   \n",
       "2813  dist q: We can find the introduction to a prod...   \n",
       "3069  dist q: What did Paul have for dinner ? a: The...   \n",
       "3096  dist q: What does Sue like ? a: Playing games ...   \n",
       "1368  dist q: Lisa in this passage is the name of a:...   \n",
       "3751  dist q: You should especially protect yourself...   \n",
       "\n",
       "                                            Actual Text  \\\n",
       "3236  [[Music, of, the, Mission, District], [The, Sp...   \n",
       "2416  [[high, school, advisers, from, Houston], [col...   \n",
       "2869  [[the, French, are, less, willing, to, buy, ec...   \n",
       "778   [[in, Europe, 94, billion, cubic, meters, of, ...   \n",
       "445   [[will, not, enjoy, the, synchronized, swimmin...   \n",
       "...                                                 ...   \n",
       "2813                  [[Part, 1], [Part, 2], [Part, 3]]   \n",
       "3069  [[The, soup, and, the, duck], [The, duck, ,, t...   \n",
       "3096                         [[Swimming, and, reading]]   \n",
       "1368                      [[a, male, lion], [a, pride]]   \n",
       "3751  [[Jet, Ski, Tours, of, Miami], [15, thStreet, ...   \n",
       "\n",
       "                                         Generated Text      bleu  \n",
       "3236                           [The, Mission, District]  0.513417  \n",
       "2416                           [high, school, students]  0.513417  \n",
       "2869  [French, holiday, shoppers, are, choosing, mor...  0.526316  \n",
       "778   [94, billion, cubic, meters, of, methane, per,...  0.530714  \n",
       "445                         [the, men's, 100, m, final]  0.536256  \n",
       "...                                                 ...       ...  \n",
       "2813                                          [Part, 3]  1.000000  \n",
       "3069                        [The, duck, and, the, soup]  1.000000  \n",
       "3096                           [Swimming, and, reading]  1.000000  \n",
       "1368                                    [a, male, lion]  1.000000  \n",
       "3751                       [Jet, Ski, Tours, of, Miami]  1.000000  \n",
       "\n",
       "[312 rows x 4 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare[dist_compare.bleu>0.5].sort_values('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de4a6eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              dist q: We can judge from the Deloitte study t...\n",
       "Actual Text       [[the, French, are, less, willing, to, buy, ec...\n",
       "Generated Text    [French, holiday, shoppers, are, choosing, mor...\n",
       "bleu                                                       0.526316\n",
       "Name: 2869, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4e74f7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dist q: We can judge from the Deloitte study that a: over a quarter of the French give second - hand Christmas gifts p: A used book or nearly - new kitchen gadget    may not be at the top of every Christmas wish list , but hard economic times coupled with a new green awareness are changing attitudes about gift - giving in France . French holiday shoppers are choosing larger numbers for \" green \" gifting this Christmas , studies show . About 30 percent of French consumers will give second - hand items as gifts to stretch out their tight budgets but also to do their little bit for recycling , according to a study by international consulting firm Deloitte . The survey of Christmas consumer behaviors in 18 European countries found the French were more than twice as likely as other Europeans to give second - hand items . Websites promoting re - gifting and green gifting are popular in France , with many reporting a rise in business . \" Concerns about the ecology and the economy have come together and we are now seeing people who accept the types of gifts that were not appreciated just a short time ago , \" said Sebastien Ravut , who runs a website promoting eco - friendly consumerism . His site lists shops in France that offer fair trade products , bio - friendly goods and recycled items . Over the Christmas holidays , the number of visits to the site has doubled from last year , reaching 60,000 a month . A study by online survey firm Vivodi for PriceMinister showed eight out of 10 people would be happy to receive a used item as a gift and that younger consumers were more open to the idea . But Gilles Goldenberg , author of the Deloitte study , said that environmental concerns are not why customers buy used goods . \" The number one concern is getting the lowest possible price , \" said Goldenberg . \" Eco - friendly products are drawing a lot of interest , but not if that means paying more . \" Theatre tickets and other low - carbon gifts are fashionable , and eco - friendly websites are also encouraging gift givers to offer time and services instead of stuff . \" The order of the day is to spend less time shopping and more time connecting \" over the holidays , said Florence de Monclin from the Nicolas Helot foundation for Nature and Humanity .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4faf8c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'French',\n",
       "  'are',\n",
       "  'less',\n",
       "  'willing',\n",
       "  'to',\n",
       "  'buy',\n",
       "  'eco',\n",
       "  '-',\n",
       "  'friendly',\n",
       "  'gifts',\n",
       "  'than',\n",
       "  'other',\n",
       "  'Europeans'],\n",
       " ['80',\n",
       "  '%',\n",
       "  'of',\n",
       "  'French',\n",
       "  'people',\n",
       "  'are',\n",
       "  'happy',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'second',\n",
       "  '-',\n",
       "  'hand',\n",
       "  'gifts'],\n",
       " ['less',\n",
       "  'than',\n",
       "  '10',\n",
       "  '%',\n",
       "  'of',\n",
       "  'European',\n",
       "  'consumers',\n",
       "  'are',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'give',\n",
       "  'second',\n",
       "  '-',\n",
       "  'hand',\n",
       "  'gifts']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['Actual Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a24a7dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['French',\n",
       " 'holiday',\n",
       " 'shoppers',\n",
       " 'are',\n",
       " 'choosing',\n",
       " 'more',\n",
       " 'than',\n",
       " 'a',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'the',\n",
       " 'French',\n",
       " 'consumers',\n",
       " 'will',\n",
       " 'give',\n",
       " 'second',\n",
       " '-',\n",
       " 'hand',\n",
       " 'items']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['Generated Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c0f2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50a5016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0978308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dist q: You should especially protect yourself from sun burnt while visiting a: American Watersports p: American Watersports     Tuesday - Saturday Located on the beach of the Sea Gardens Beach Resort , there are fun things to rent for the whole family . They offer rentals for kayaks , jet skis , and even parasail ! Enjoy the water up - close or from a bird 's eye view ! No matter what activity you 're enjoying , be sure to protect yourself and your family from the sun 's powerful rays and apply plenty of sun block ! 15thStreet Boat Company Monday - Saturday 15thStreet Boat Company offers rental boats of all kinds . They 're sure to have what you are looking for , whether it 's a small boat for a quick and simple outing or an extravagant boat with comfortable seats with a stereo and high tech navigation . You can rent a boat for half a day or a couple of days , or even weeks at a time . If you want it , they 've got it . It 's fun for everyone ! Coconut 's Watersports      9am-5pm Monday through Sunday Coconut 's Watersports is open 7 days a week for your convenience and offers tons of water fun for the whole family . Jet Ski activities last 30 minutes or 1 hour and can make stops along the way . You must be at least 14 years of age to ride alone and can be as young as 3 to ride along with an adult . Everybody is required to wear a life jacket and a license is required to operate the Jet Ski . Bathing suits and shorts are recommended . Jet Ski Tours of Miami      Thursday - Sunday 10am-7pm Jet Ski Tours of Miami includes onsite parking , indoor restrooms , lockers , and life jackets for participants . You may choose a one or two tour and each Jet Ski can hold up to 3 people . You must be at least 18 years old in order to ride . As long as you are accompanied by an adult , there is no age limitation for any passenger . There is a restaurant nearby to eat at . The tour visits 6 different islands and passes by Bayside and Hard Rock . You may even catch a glimpse of dolphins or a manatee resting in these fabulous Florida waters .\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[3751].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be4b5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>dist q: The Dixie PIT program was introduced i...</td>\n",
       "      <td>[raise money for school affairs, supply teache...</td>\n",
       "      <td>.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3554</th>\n",
       "      <td>dist q: Which of the following sentences from ...</td>\n",
       "      <td>[In her calm , motherly voice she said,\"By the...</td>\n",
       "      <td>''</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dist q: A \" property \" in Australia is a a: fa...</td>\n",
       "      <td>[school]</td>\n",
       "      <td>farm</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>dist q: If you want to be a member of Summer S...</td>\n",
       "      <td>[visit Black Rock Forest first]</td>\n",
       "      <td>8455344517</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>dist q: What is the best title for the text ? ...</td>\n",
       "      <td>[Put the glass down]</td>\n",
       "      <td>\"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>dist q: According to the author , which of the...</td>\n",
       "      <td>[Western women, Chinese men]</td>\n",
       "      <td>Western women</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>dist q: You should especially protect yourself...</td>\n",
       "      <td>[Jet Ski Tours of Miami, 15 \\n thStreet Boat C...</td>\n",
       "      <td>Jet Ski Tours of Miami</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>dist q: The old woman wants to see a: Xiao Min...</td>\n",
       "      <td>[Xiao Ming]</td>\n",
       "      <td>Xiao Ming</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>dist q: What is the best title for the passage...</td>\n",
       "      <td>[Pop music, Classical music, Folk music]</td>\n",
       "      <td>Classical music</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>dist q: Which can be the best title of this pa...</td>\n",
       "      <td>[How to Live Simply, My Grandparents]</td>\n",
       "      <td>My Grandparents</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3773 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1701  dist q: The Dixie PIT program was introduced i...   \n",
       "3554  dist q: Which of the following sentences from ...   \n",
       "37    dist q: A \" property \" in Australia is a a: fa...   \n",
       "1026  dist q: If you want to be a member of Summer S...   \n",
       "3155  dist q: What is the best title for the text ? ...   \n",
       "...                                                 ...   \n",
       "152   dist q: According to the author , which of the...   \n",
       "3751  dist q: You should especially protect yourself...   \n",
       "2218  dist q: The old woman wants to see a: Xiao Min...   \n",
       "3147  dist q: What is the best title for the passage...   \n",
       "3416  dist q: Which can be the best title of this pa...   \n",
       "\n",
       "                                            Actual Text  \\\n",
       "1701  [raise money for school affairs, supply teache...   \n",
       "3554  [In her calm , motherly voice she said,\"By the...   \n",
       "37                                             [school]   \n",
       "1026                    [visit Black Rock Forest first]   \n",
       "3155                               [Put the glass down]   \n",
       "...                                                 ...   \n",
       "152                        [Western women, Chinese men]   \n",
       "3751  [Jet Ski Tours of Miami, 15 \\n thStreet Boat C...   \n",
       "2218                                        [Xiao Ming]   \n",
       "3147           [Pop music, Classical music, Folk music]   \n",
       "3416              [How to Live Simply, My Grandparents]   \n",
       "\n",
       "              Generated Text  bleu  \n",
       "1701                       .   0.0  \n",
       "3554                      ''   0.0  \n",
       "37                      farm   0.0  \n",
       "1026              8455344517   0.0  \n",
       "3155                       \"   0.0  \n",
       "...                      ...   ...  \n",
       "152            Western women   1.0  \n",
       "3751  Jet Ski Tours of Miami   1.0  \n",
       "2218               Xiao Ming   1.0  \n",
       "3147         Classical music   1.0  \n",
       "3416         My Grandparents   1.0  \n",
       "\n",
       "[3773 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.sort_values('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a795529d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your eyes are poor',\n",
       " 'your left eye is not open',\n",
       " 'you move it close to your eye']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[675,:]['Actual Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb61db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dist q: You fail to see the letter L in the experiment because a: its image falls on the blind spot p: It seems to be strange to you there is a blind spot on the eyes , Here is an interesting experiment that can make something disappear , when one eye is open . Make a card about the size of a postcard and write two English letters L and R on it , L on the left and R on the right . First , hold the card about 80 cm away and you see both the letters . Then close your right eye and look at the letter R only with your left eye . And now , as you move the card slowly towards you , you'll find the letter L disappearing . But if you move the card nearer to your face , the letter will be seen again . Now do the same experiment with your left eye closed , you'll find the letter R disappearing . Why does the letter disappear ? It is because there is a blind spot on the eye . When the image of the letter falls on the blind spot , it wo n't be seen . That is why either of the letters disappears .\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[675,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71f6f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/cluster/home/fgonzalez/.virtualenvs/nlp/lib64/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist_compare=val.merge(gen_dist,on=['text'])\n",
    "aa=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0)),axis=1)\n",
    "dist_compare=dist_compare.assign(bleu=aa)\n",
    "#dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "bleu_3=dist_compare.bleu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "226874f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.206983827137286e-308"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2c06d",
   "metadata": {},
   "source": [
    "## scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddaf608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21606667354214598"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ea36bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07887480099137983"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9660b8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04384669033424478"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b01b244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033407103702946084"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(Configuration.parse_cmd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
