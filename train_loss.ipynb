{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d83841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "from configuration import Configuration\n",
    "from configuration import CONSTANTS as C\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch import cuda\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92df0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "    \"\"\"    \n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len,answer_len, source_text, target_text,answer_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.ans_len = answer_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "        self.answer_text = self.data[answer_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "        answer_text = str(self.answer_text[index])\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "        answer_text = ' '.join(answer_text.split())\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        answer = self.tokenizer.batch_encode_plus([answer_text], max_length= self.ans_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "        answer_ids = answer['input_ids'].squeeze()\n",
    "        answer_mask = answer['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long),\n",
    "            'answer_ids': answer_ids.to(dtype=torch.long),\n",
    "            'answer_mask': answer_mask.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_model_dir(experiment_main_dir, experiment_id, model_summary):\n",
    "    \"\"\"\n",
    "    Create a new model directory.\n",
    "    :param experiment_main_dir: Where all experiments are stored.\n",
    "    :param experiment_id: The ID of this experiment.\n",
    "    :param model_summary: A summary string of the model.\n",
    "    :return: A directory where we can store model logs. Raises an exception if the model directory already exists.\n",
    "    \"\"\"\n",
    "    model_name = \"{}-{}\".format(experiment_id, model_summary)\n",
    "    model_dir = os.path.join(experiment_main_dir, model_name)\n",
    "    #if os.path.exists(model_dir):\n",
    "    #    raise ValueError(\"Model directory already exists {}\".format(model_dir))\n",
    "    #os.makedirs(model_dir)\n",
    "    return model_dir\n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer,writer,global_step,records):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    c=0\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        ans_str = data['answer_ids'].to(device, dtype = torch.long)\n",
    "        ans_mask = data['answer_mask'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids,\n",
    "                        labels=lm_labels,answer_str=ans_str,answer_mask=ans_mask)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"loss\", loss, global_step)\n",
    "        \n",
    "        \n",
    "        ### measure bleu\n",
    "        if c%10==0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            actuals = []\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "\n",
    "            temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "\n",
    "            val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "            gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "            distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "            dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "            dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()\n",
    "            aa=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "            dist_compare=dist_compare.assign(bleu=aa)\n",
    "            bleu_2=dist_compare.bleu.mean()\n",
    "            if c%1000==0:\n",
    "                path = os.path.join(model_dir, \"model_files\")\n",
    "                model.save_pretrained(path)\n",
    "                tokenizer.save_pretrained(path)\n",
    "\n",
    "            model.train()\n",
    "            writer.add_scalar(\"bleu2\", bleu_2, global_step)\n",
    "        \n",
    "        \n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def validate(epoch, tokenizer, model, device, loader,writer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    global_step = 0\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    c=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            c=c+1\n",
    "            if c%100==0:\n",
    "                print(c)\n",
    "        temp_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "\n",
    "        val=records.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "        gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']]\n",
    "\n",
    "        distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "        dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "        dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()\n",
    "        aa=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "        dist_compare=dist_compare.assign(bleu=aa)\n",
    "        bleu_2=dist_compare.bleu.mean()\n",
    "        writer.add_scalar(\"bleu2_val\", bleu_2, 1)\n",
    "        \n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3833330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    model_params={\n",
    "        \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "        \"TRAIN_BATCH_SIZE\":4,          # training batch size\n",
    "        \"VALID_BATCH_SIZE\":4,          # validation batch size\n",
    "        \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "        \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "        \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "        \"MAX_SOURCE_TEXT_LENGTH\":1800,  # max length of source text\n",
    "        \"MAX_TARGET_TEXT_LENGTH\":200,   # max length of target text\n",
    "        \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    source_text='text'\n",
    "    target_text='distractor'\n",
    "    model_params=model_params\n",
    "\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_train_original.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records = json.loads(content)\n",
    "    records=pd.DataFrame(records)\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(C.DEVICE)\n",
    "\n",
    "    ## format the input\n",
    "    records=records.assign(question=records.question.str.join(' '))\n",
    "    records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "    records=records.assign(article=records.article.str.join(' '))\n",
    "    records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "    records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "    records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "    records=records.loc[:,['text','distractor']]\n",
    "\n",
    "    with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_original.json\"), 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "    content=content.replace('\\n',',')\n",
    "    content='['+content[:-1]+']'\n",
    "    records_test = json.loads(content)\n",
    "    records_test=pd.DataFrame(records_test)\n",
    "\n",
    "    ## format the input\n",
    "    records_test=records_test.assign(question=records_test.question.str.join(' '))\n",
    "    records_test=records_test.assign(distractor=records_test.distractor.str.join(' '))\n",
    "    records_test=records_test.assign(article=records_test.article.str.join(' '))\n",
    "    records_test=records_test.assign(answer_text=records_test.answer_text.str.join(' '))\n",
    "    records_test=records_test.loc[:,['article','question','answer_text','distractor']]\n",
    "    records_test=records_test.assign(text=\"distraction passage: \"+records_test.article+\" question: \"+records_test.question+\" answer: \"+records_test.answer_text)\n",
    "    records_test=records_test.loc[:,['text','distractor']]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    val_dataset=records_test\n",
    "    train_dataset = records\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "    \n",
    "    # Create Tensorboard logger.\n",
    "    experiment_id = int(time.time())\n",
    "    experiment_name = \"name\"\n",
    "    model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "        \n",
    "    global_step = 0\n",
    "    writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step,records)\n",
    "\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(model_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa9f158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-small\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":2,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":2,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":900,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":901,   # max length of target text\n",
    "    \"MAX_ANSWER_LENGTH\":900,   # max length of answer text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "source_text='text'\n",
    "target_text='distractor'\n",
    "answer_text='answer_text'\n",
    "model_params=model_params\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_train_original.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records = json.loads(content)\n",
    "records=pd.DataFrame(records)\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "model = model.to(C.DEVICE)\n",
    "\n",
    "## format the input\n",
    "records=records.assign(question=records.question.str.join(' '))\n",
    "records=records.assign(distractor=records.distractor.str.join(' '))\n",
    "records=records.assign(article=records.article.str.join(' '))\n",
    "records=records.assign(answer_text=records.answer_text.str.join(' '))\n",
    "records=records.loc[:,['article','question','answer_text','distractor']]\n",
    "records=records.assign(text=\"distraction passage: \"+records.article+\" question: \"+records.question+\" answer: \"+records.answer_text)\n",
    "records=records.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "with open(os.path.join(C.DATA_DIR, \"distractor/race_dev_original.json\"), 'r') as content_file:\n",
    "    content = content_file.read()\n",
    "content=content.replace('\\n',',')\n",
    "content='['+content[:-1]+']'\n",
    "records_test = json.loads(content)\n",
    "records_test=pd.DataFrame(records_test)\n",
    "\n",
    "## format the input\n",
    "records_test=records_test.assign(question=records_test.question.str.join(' '))\n",
    "records_test=records_test.assign(distractor=records_test.distractor.str.join(' '))\n",
    "records_test=records_test.assign(article=records_test.article.str.join(' '))\n",
    "records_test=records_test.assign(answer_text=records_test.answer_text.str.join(' '))\n",
    "records_test=records_test.loc[:,['article','question','answer_text','distractor']]\n",
    "records_test=records_test.assign(text=\"distraction passage: \"+records_test.article+\" question: \"+records_test.question+\" answer: \"+records_test.answer_text)\n",
    "records_test=records_test.loc[:,['text','distractor','answer_text']]\n",
    "\n",
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "val_dataset=records_test\n",
    "train_dataset = records\n",
    "\n",
    "\n",
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"],model_params[\"MAX_ANSWER_LENGTH\"], source_text, target_text,answer_text)\n",
    "\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "  'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "  'shuffle': True,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "val_params = {\n",
    "  'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "  'shuffle': False,\n",
    "  'num_workers': 0\n",
    "  }\n",
    "\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "# Create Tensorboard logger.\n",
    "experiment_id =55555 #int(time.time())\n",
    "experiment_name = \"name\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = create_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), experiment_id, experiment_name)\n",
    "\n",
    "global_step = 0\n",
    "writer = SummaryWriter(os.path.join(model_dir, 'logs'))\n",
    "for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "    global_step=train(epoch, tokenizer, model, C.DEVICE, training_loader, optimizer,writer,global_step,records)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d03160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model after training\n",
    "path = os.path.join(model_dir, \"model_files\")\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "# evaluating test dataset\n",
    "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0366eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dir(experiment_dir, model_id):\n",
    "    \"\"\"Return the directory in `experiment_dir` that contains the given `model_id` string.\"\"\"\n",
    "    model_dir = glob.glob(os.path.join(experiment_dir, str(model_id) + \"-*\"), recursive=False)\n",
    "    return None if len(model_dir) == 0 else model_dir[0]\n",
    "\n",
    "def get_model_config(model_id):\n",
    "    model_id = model_id\n",
    "    model_dir = get_model_dir(os.path.join(C.DATA_DIR, \"experiments/\"), model_id)\n",
    "    model_config = 0#Configuration.from_json(os.path.join(model_dir, 'config.json'))\n",
    "    return model_config, model_dir\n",
    "\n",
    "def load_model(model_id):\n",
    "    model_config, model_dir = get_model_config(model_id)\n",
    "    path = os.path.join(model_dir, \"model_files\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "\n",
    "    model.to(C.DEVICE)\n",
    "\n",
    "    return model,tokenizer, model_config, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61949b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer, model_config, model_dir = load_model(1622411389)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62761b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e7111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join(model_dir, 'logs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1558cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, C.DEVICE, val_loader,writer)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(model_dir, 'predictions.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6968c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.read_csv(os.path.join(model_dir, 'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff21de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_test_fil=records_test[~records_test.text.isin(records.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc772f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = final_df\n",
    "\n",
    "val=records_test_fil.rename(columns={'distractor':'Actual Text'})\n",
    "\n",
    "gen_dist=val.merge(temp_df,on=['Actual Text']).loc[:,['text','Generated Text']].drop_duplicates()\n",
    "\n",
    "distractors=val.groupby(['text']).agg({ 'Actual Text': lambda x: list(x.str.split())}).reset_index()\n",
    "\n",
    "dist_compare=distractors.merge(gen_dist,on=['text'])\n",
    "\n",
    "dist_compare['Generated Text']=dist_compare['Generated Text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca08a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(1, 0, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b2=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 1, 0, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b3=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "b4=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 0, 1),smoothing_function=SmoothingFunction().method1),axis=1)\n",
    "dist_compare=dist_compare.assign(bleu1=b1)\n",
    "dist_compare=dist_compare.assign(bleu2=b2)\n",
    "dist_compare=dist_compare.assign(bleu3=b3)\n",
    "dist_compare=dist_compare.assign(bleu4=b4)\n",
    "#dist_compare=dist_compare.assign(bleu=dist_compare.apply(lambda x:sentence_bleu(x['Actual Text'],x['Generated Text'],weights=(0, 0, 1, 0)),axis=1))\n",
    "bleu_1=dist_compare.bleu1.mean()\n",
    "bleu_2=dist_compare.bleu2.mean()\n",
    "bleu_3=dist_compare.bleu3.mean()\n",
    "bleu_4=dist_compare.bleu4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91598d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03404204049848772"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427a195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>distraction passage: In all one 's lifetime it...</td>\n",
       "      <td>[[not, to, be, too, proud], [not, to, go, down...</td>\n",
       "      <td>[one's, life, is, full, of, color, and, flavor]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>distraction passage: Interactive television ad...</td>\n",
       "      <td>[[showed, an, indifferent, attitude, toward, it]]</td>\n",
       "      <td>[a, new, advertising, agency, has, been, rolli...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>distraction passage: When we donate   blood , ...</td>\n",
       "      <td>[[O]]</td>\n",
       "      <td>[AB, is, the, universal, receiver]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>distraction passage: Instead of hitting the be...</td>\n",
       "      <td>[[paid, for, their, research], [found, way, to...</td>\n",
       "      <td>[students'interest, in, science, and, agricult...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>distraction passage: Inprefix = st1 /Kansas Ci...</td>\n",
       "      <td>[[the, City, Hall], [the, centre, of, the, city]]</td>\n",
       "      <td>[900, fire, fighters]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>distraction passage: Money and Happiness A Gui...</td>\n",
       "      <td>[[Money, and, Happiness], [The, Happiness, Mak...</td>\n",
       "      <td>[The, Happiness, Makeover]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>distraction passage: In one study , college st...</td>\n",
       "      <td>[[substantive, talks, make, people, happier, t...</td>\n",
       "      <td>[small, talks, are, important, in, communication]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>distraction passage: Decisions , decisions ! O...</td>\n",
       "      <td>[[emotions, are, the, enemy, of, decision, mak...</td>\n",
       "      <td>[emotions, are, the, enemy, of, decision, making]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>distraction passage: School cleaning day Today...</td>\n",
       "      <td>[[cleaning, the, windows], [sweeping, the, flo...</td>\n",
       "      <td>[cleaning, the, windows]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>distraction passage: Most Chinese people do n'...</td>\n",
       "      <td>[[choose, to, eat, meat, or, fish], [choose, n...</td>\n",
       "      <td>[eat, meat, or, fish]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1521  distraction passage: In all one 's lifetime it...   \n",
       "1611  distraction passage: Interactive television ad...   \n",
       "3566  distraction passage: When we donate   blood , ...   \n",
       "1609  distraction passage: Instead of hitting the be...   \n",
       "1606  distraction passage: Inprefix = st1 /Kansas Ci...   \n",
       "...                                                 ...   \n",
       "2049  distraction passage: Money and Happiness A Gui...   \n",
       "1553  distraction passage: In one study , college st...   \n",
       "779   distraction passage: Decisions , decisions ! O...   \n",
       "2591  distraction passage: School cleaning day Today...   \n",
       "2072  distraction passage: Most Chinese people do n'...   \n",
       "\n",
       "                                            Actual Text  \\\n",
       "1521  [[not, to, be, too, proud], [not, to, go, down...   \n",
       "1611  [[showed, an, indifferent, attitude, toward, it]]   \n",
       "3566                                              [[O]]   \n",
       "1609  [[paid, for, their, research], [found, way, to...   \n",
       "1606  [[the, City, Hall], [the, centre, of, the, city]]   \n",
       "...                                                 ...   \n",
       "2049  [[Money, and, Happiness], [The, Happiness, Mak...   \n",
       "1553  [[substantive, talks, make, people, happier, t...   \n",
       "779   [[emotions, are, the, enemy, of, decision, mak...   \n",
       "2591  [[cleaning, the, windows], [sweeping, the, flo...   \n",
       "2072  [[choose, to, eat, meat, or, fish], [choose, n...   \n",
       "\n",
       "                                         Generated Text  bleu1  bleu2  bleu3  \\\n",
       "1521    [one's, life, is, full, of, color, and, flavor]    0.0    0.0   0.00   \n",
       "1611  [a, new, advertising, agency, has, been, rolli...    0.0    0.0   0.00   \n",
       "3566                 [AB, is, the, universal, receiver]    0.0    0.0   0.00   \n",
       "1609  [students'interest, in, science, and, agricult...    0.0    0.0   0.00   \n",
       "1606                              [900, fire, fighters]    0.0    0.0   0.00   \n",
       "...                                                 ...    ...    ...    ...   \n",
       "2049                         [The, Happiness, Makeover]    1.0    1.0   1.00   \n",
       "1553  [small, talks, are, important, in, communication]    1.0    1.0   0.75   \n",
       "779   [emotions, are, the, enemy, of, decision, making]    1.0    1.0   1.00   \n",
       "2591                           [cleaning, the, windows]    1.0    1.0   1.00   \n",
       "2072                              [eat, meat, or, fish]    1.0    1.0   1.00   \n",
       "\n",
       "         bleu4  \n",
       "1521  0.000000  \n",
       "1611  0.000000  \n",
       "3566  0.000000  \n",
       "1609  0.000000  \n",
       "1606  0.000000  \n",
       "...        ...  \n",
       "2049  0.100000  \n",
       "1553  0.666667  \n",
       "779   1.000000  \n",
       "2591  0.100000  \n",
       "2072  1.000000  \n",
       "\n",
       "[3781 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.sort_values('bleu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5aef4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Actual Text</th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>distraction passage: I 've often had difficult...</td>\n",
       "      <td>[[The, writer, had, difficulty, remembering, n...</td>\n",
       "      <td>[Billerica, was, the, name, of, a, town, in, B...</td>\n",
       "      <td>0.711767</td>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.343173</td>\n",
       "      <td>0.266912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>distraction passage: Each Sunday , people can ...</td>\n",
       "      <td>[[Music, of, the, Mission, District], [The, Sp...</td>\n",
       "      <td>[the, Mission, District]</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.051342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>distraction passage: High school could be a sc...</td>\n",
       "      <td>[[how, to, take, art, classes], [the, fun, sid...</td>\n",
       "      <td>[high, school, life]</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.051342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>distraction passage: Fruit salad is a deliciou...</td>\n",
       "      <td>[[fruit, salad, is, very, delicious], [people,...</td>\n",
       "      <td>[eat, fruit, salad]</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.051342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>distraction passage: To be a good teacher , yo...</td>\n",
       "      <td>[[How, to, be, a, good, actor], [A, good, teac...</td>\n",
       "      <td>[a, good, teacher]</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.051342</td>\n",
       "      <td>0.051342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>distraction passage: For kids and many adults ...</td>\n",
       "      <td>[[San, Diego, Zoo], [Maritime, Museum, of, San...</td>\n",
       "      <td>[Maritime, Museum, of, San, Diego]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>distraction passage: Decisions , decisions ! O...</td>\n",
       "      <td>[[emotions, are, the, enemy, of, decision, mak...</td>\n",
       "      <td>[emotions, are, the, enemy, of, decision, making]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>distraction passage: Around the world there ar...</td>\n",
       "      <td>[[the, world, championship, of, grimaces], [th...</td>\n",
       "      <td>[the, world, championship, of, grimaces]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>distraction passage: Singapore is a dynamic ci...</td>\n",
       "      <td>[[9:00, am, to, 11:00, am], [11:30, am, to, 1:...</td>\n",
       "      <td>[2:30, pm, to, 4:30, pm]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>distraction passage: Livescribe Echo Smartpen ...</td>\n",
       "      <td>[[ILUV, Syren, Pro]]</td>\n",
       "      <td>[ILUV, Syren, Pro]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1288  distraction passage: I 've often had difficult...   \n",
       "871   distraction passage: Each Sunday , people can ...   \n",
       "1200  distraction passage: High school could be a sc...   \n",
       "1072  distraction passage: Fruit salad is a deliciou...   \n",
       "3210  distraction passage: To be a good teacher , yo...   \n",
       "...                                                 ...   \n",
       "1038  distraction passage: For kids and many adults ...   \n",
       "779   distraction passage: Decisions , decisions ! O...   \n",
       "457   distraction passage: Around the world there ar...   \n",
       "2658  distraction passage: Singapore is a dynamic ci...   \n",
       "1896  distraction passage: Livescribe Echo Smartpen ...   \n",
       "\n",
       "                                            Actual Text  \\\n",
       "1288  [[The, writer, had, difficulty, remembering, n...   \n",
       "871   [[Music, of, the, Mission, District], [The, Sp...   \n",
       "1200  [[how, to, take, art, classes], [the, fun, sid...   \n",
       "1072  [[fruit, salad, is, very, delicious], [people,...   \n",
       "3210  [[How, to, be, a, good, actor], [A, good, teac...   \n",
       "...                                                 ...   \n",
       "1038  [[San, Diego, Zoo], [Maritime, Museum, of, San...   \n",
       "779   [[emotions, are, the, enemy, of, decision, mak...   \n",
       "457   [[the, world, championship, of, grimaces], [th...   \n",
       "2658  [[9:00, am, to, 11:00, am], [11:30, am, to, 1:...   \n",
       "1896                               [[ILUV, Syren, Pro]]   \n",
       "\n",
       "                                         Generated Text     bleu1     bleu2  \\\n",
       "1288  [Billerica, was, the, name, of, a, town, in, B...  0.711767  0.500461   \n",
       "871                            [the, Mission, District]  0.513417  0.513417   \n",
       "1200                               [high, school, life]  0.513417  0.513417   \n",
       "1072                                [eat, fruit, salad]  0.513417  0.513417   \n",
       "3210                                 [a, good, teacher]  0.513417  0.513417   \n",
       "...                                                 ...       ...       ...   \n",
       "1038                 [Maritime, Museum, of, San, Diego]  1.000000  1.000000   \n",
       "779   [emotions, are, the, enemy, of, decision, making]  1.000000  1.000000   \n",
       "457            [the, world, championship, of, grimaces]  1.000000  1.000000   \n",
       "2658                           [2:30, pm, to, 4:30, pm]  1.000000  1.000000   \n",
       "1896                                 [ILUV, Syren, Pro]  1.000000  1.000000   \n",
       "\n",
       "         bleu3     bleu4  \n",
       "1288  0.343173  0.266912  \n",
       "871   0.513417  0.051342  \n",
       "1200  0.513417  0.051342  \n",
       "1072  0.513417  0.051342  \n",
       "3210  0.051342  0.051342  \n",
       "...        ...       ...  \n",
       "1038  1.000000  1.000000  \n",
       "779   1.000000  1.000000  \n",
       "457   1.000000  1.000000  \n",
       "2658  1.000000  1.000000  \n",
       "1896  1.000000  0.100000  \n",
       "\n",
       "[73 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare[dist_compare.bleu2>0.5].sort_values('bleu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de4a6eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              dist q: We can judge from the Deloitte study t...\n",
       "Actual Text       [[the, French, are, less, willing, to, buy, ec...\n",
       "Generated Text    [French, holiday, shoppers, are, choosing, mor...\n",
       "bleu                                                       0.526316\n",
       "Name: 2869, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557fd8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dist q: We can judge from the Deloitte study that a: over a quarter of the French give second - hand Christmas gifts p: A used book or nearly - new kitchen gadget    may not be at the top of every Christmas wish list , but hard economic times coupled with a new green awareness are changing attitudes about gift - giving in France . French holiday shoppers are choosing larger numbers for \" green \" gifting this Christmas , studies show . About 30 percent of French consumers will give second - hand items as gifts to stretch out their tight budgets but also to do their little bit for recycling , according to a study by international consulting firm Deloitte . The survey of Christmas consumer behaviors in 18 European countries found the French were more than twice as likely as other Europeans to give second - hand items . Websites promoting re - gifting and green gifting are popular in France , with many reporting a rise in business . \" Concerns about the ecology and the economy have come together and we are now seeing people who accept the types of gifts that were not appreciated just a short time ago , \" said Sebastien Ravut , who runs a website promoting eco - friendly consumerism . His site lists shops in France that offer fair trade products , bio - friendly goods and recycled items . Over the Christmas holidays , the number of visits to the site has doubled from last year , reaching 60,000 a month . A study by online survey firm Vivodi for PriceMinister showed eight out of 10 people would be happy to receive a used item as a gift and that younger consumers were more open to the idea . But Gilles Goldenberg , author of the Deloitte study , said that environmental concerns are not why customers buy used goods . \" The number one concern is getting the lowest possible price , \" said Goldenberg . \" Eco - friendly products are drawing a lot of interest , but not if that means paying more . \" Theatre tickets and other low - carbon gifts are fashionable , and eco - friendly websites are also encouraging gift givers to offer time and services instead of stuff . \" The order of the day is to spend less time shopping and more time connecting \" over the holidays , said Florence de Monclin from the Nicolas Helot foundation for Nature and Humanity .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f042f5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'French',\n",
       "  'are',\n",
       "  'less',\n",
       "  'willing',\n",
       "  'to',\n",
       "  'buy',\n",
       "  'eco',\n",
       "  '-',\n",
       "  'friendly',\n",
       "  'gifts',\n",
       "  'than',\n",
       "  'other',\n",
       "  'Europeans'],\n",
       " ['80',\n",
       "  '%',\n",
       "  'of',\n",
       "  'French',\n",
       "  'people',\n",
       "  'are',\n",
       "  'happy',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'second',\n",
       "  '-',\n",
       "  'hand',\n",
       "  'gifts'],\n",
       " ['less',\n",
       "  'than',\n",
       "  '10',\n",
       "  '%',\n",
       "  'of',\n",
       "  'European',\n",
       "  'consumers',\n",
       "  'are',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'give',\n",
       "  'second',\n",
       "  '-',\n",
       "  'hand',\n",
       "  'gifts']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['Actual Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7f224b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['French',\n",
       " 'holiday',\n",
       " 'shoppers',\n",
       " 'are',\n",
       " 'choosing',\n",
       " 'more',\n",
       " 'than',\n",
       " 'a',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'the',\n",
       " 'French',\n",
       " 'consumers',\n",
       " 'will',\n",
       " 'give',\n",
       " 'second',\n",
       " '-',\n",
       " 'hand',\n",
       " 'items']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_compare.iloc[2869]['Generated Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2c06d",
   "metadata": {},
   "source": [
    "## scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddaf608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18203008418568886"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea36bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05949882910240971"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9660b8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03404204049848772"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b01b244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026776927356500054"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a0208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(Configuration.parse_cmd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
